{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 범용적 목적의 분산 고성능 클러스터링 **플랫폼**\n",
    "- 대용량 데이터 프로세싱을 위한 빠르고 범용적인 인메모리 기반 클러스터 컴퓨팅 엔진\n",
    "- 분산 메모리 기반의 바른 분산 병렬 처리\n",
    "- Scala, Java, Python, R 기반 API 제공\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = http://spark.apache.org/images/spark-stack.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 특징\n",
    "- Unified Engine : 통합적 엔진으로, 1개의 플랫폼에서 다양한 기능을 수행할 수 있음\n",
    "- High-level APIs : 쉽게 사용할 수 있는 양질의 API 존재\n",
    "- Integrated Broadly : Streaming / SQL / ML / Graph / Batch 등 다양한 기능을 통합함\n",
    "\n",
    "\n",
    "- In-Memory 컴퓨팅\n",
    "- RDD ( Resilient Distributed Dataset ) 데이터 모델\n",
    "- General execution graphs => DAG ( Directed Acyclic Graph ) => Multiple stages of map & reduce\n",
    "- Hadoop과 유연한 연계\n",
    "- 대화형 질의를 위한 Interactive Shell 지원\n",
    "- 실시간 ( Real-time ) Stream Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD ( Resilient Distributed Dataset )\n",
    "\n",
    "- Dataset : 메모리나 디스크에 분산 저장된 **변경 불가능한** 데이터 객체들의 모음 (-> 변경하면 새로운 set으로 저장)\n",
    "- Distributed : RDD에 있는 데이터는 클러스터에 자동 분배 및 병렬 연산 수행\n",
    "- Resilient : 클러스터의 한 노드아 실패하더라도 다른 노드가 작업 처리해 복원 가능\n",
    "- Immutable : RDD는 수정이 안됨. 변형을 통한 새로운 RDD 생성\n",
    "- Operaction APIs\n",
    "    * Transformation ( 데이터 변형. ex) map, filter, groupBy, join)\n",
    "    * Actions ( 결과 연산 리턴/저장. ex) count, collect, save)\n",
    "- Lazy Evaluation : All Transformations ( Action 실행시 변형 시작 )\n",
    "- COntrollable Persistence : Cache in RAM/DISK 가능 ( 반복 연산에 유리함 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD 생성 → RDD 변형 → RDD 연산의 순서로 진행\n",
    "<img src = \"http://image.slidesharecdn.com/spark-overview-june2014-140711131800-phpapp01/95/apache-spark-hadoop-39-638.jpg?cb=1425492160\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"..\")\n",
    "lines.filter(lambda x: \"ERROR\" in x).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interactive Shell을 사용하고 싶다면, Bash, CMD, Anaconda Prompt에서 pyspark를 치면 됩니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 드라이버 노드의 경우\n",
    "\n",
    "http://localhost:4040"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Software Components\n",
    "- Spark Driver : Spark Context를 생성하는 Client단의 어플리케이션\n",
    "- Spkar Context : driver와 Cluaster Manager와 교류를 통해 Spark executors에게 전달. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://spark.apache.org/docs/latest/img/cluster-overview.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
